{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37d008b",
   "metadata": {},
   "source": [
    "# Data Science (NLP) internship Kudosware\n",
    "\n",
    "## Task: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a8ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required liraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from string import punctuation\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9678b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the CSV file\n",
    "data = pd.read_csv('data.csv', header=None, encoding='ISO-8859-1',names=['target', 'id', 'date', 'flag', 'user', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7351f409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae761e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a set of stop words for English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initializing the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185b4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing data\n",
    "def preprocess(text):\n",
    "    # Removing URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Removing mentions\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    \n",
    "    # Removing hashtags\n",
    "    text = text.replace('#', '')\n",
    "    \n",
    "    # Removing emojis and converting them to textual representation\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Removing punctuation\n",
    "    text = ''.join(char for char in text if char not in punctuation)\n",
    "    \n",
    "    # lemmatize, remove stop words, converting to lower case\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in text.split() if word.lower() not in stop_words]\n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d67b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying preprocess function\n",
    "data['text'] = data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70514148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          awww thats bummer shoulda got david carr third...\n",
       "1          upset cant update facebook texting might cry r...\n",
       "2          dived many time ball managed save 50 rest go b...\n",
       "3                            whole body feel itchy like fire\n",
       "4                                   behaving im mad cant see\n",
       "                                 ...                        \n",
       "1599995                        woke school best feeling ever\n",
       "1599996           thewdbcom cool hear old walt interview â«\n",
       "1599997                       ready mojo makeover ask detail\n",
       "1599998    happy 38th birthday boo alll time tupac amaru ...\n",
       "1599999                                 happy charitytuesday\n",
       "Name: text, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add42233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e947f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the TF-IDF vectorizer to convert text to vectors\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "train_features = vectorizer.fit_transform(train_data['text'])\n",
    "test_features = vectorizer.transform(test_data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f4433b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_features, train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56b68f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation\n",
    "def generate_tweet(seed_sentence, n=10):\n",
    "    current_sentence = seed_sentence\n",
    "    perplexity = 0\n",
    "    for i in range(n):\n",
    "        vectorized_sentence = vectorizer.transform([current_sentence])\n",
    "        prediction = clf.predict(vectorized_sentence)[0]\n",
    "        if prediction == 0:\n",
    "            next_word = np.random.choice(train_data[train_data['target'] == 0]['text'])\n",
    "        else:\n",
    "            next_word = np.random.choice(train_data[train_data['target'] == 4]['text'])\n",
    "        current_sentence += ' ' + next_word\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        prob = clf.predict_proba(vectorized_sentence)\n",
    "        perplexity += math.log(prob[0][prediction])\n",
    "    \n",
    "    # Calculate average perplexity\n",
    "    avg_perplexity = math.exp(-perplexity/n)\n",
    "    return current_sentence, avg_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "038e3eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regenerated tweet:  do you have goddamnit technology hate couldnt tweet anymore dd nighttttt noooooo lol nah made plan wen annie told late studying portwoods exam tomorrow finished typing 30 recipe 1st hour made 17 page poor tree updated myspace pgi need find friend going long nightagain wont camera july 2 owww didnt think would much pain im already fucked frustrated annoyed make smile careful dnt know heat index sa supposed another triple digit day 101 dont think going make tonight girl best would proud rode river circuit asthma hit hard wet mow amazed amused wpac cr limit raised celebrated cole shop etc\n"
     ]
    }
   ],
   "source": [
    "# Implementation\n",
    "seed_sentence = \"do you have\"\n",
    "generated_tweet, perplexity = generate_tweet(seed_sentence)\n",
    "print(\"Regenerated tweet: \", generated_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f76125ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity score:  1.225898095534193\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity score: \", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b735e8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
